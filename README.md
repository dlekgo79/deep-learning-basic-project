# deep-learning-basic-project
task: 10~50대 얼굴 이미지 분류 모델 만들기  
  
이번 프로젝트에서 배운 점은 overfitting 해결법이다.    
먼저 과적합을 해결하기 위해서 dropout과 batch normalization을 추가하여 모델을 구성하였고 optimizer에 regularization을 주기 위하여 weight decay를 추가하였습니다.
1)	dropout – 뉴런을 무작위로 제거를 하여 정규화의 효과를 주는 것이다 
dropout을 강하게 줄수록 규제가 강하게 dropout을 약하게 줄수록 규제가 약하게 적용이 되고 0.5를 보편적으로 사용한다. 성능을 올리기 위해 과적합이 나면 올리고 언더피팅의 경우 없애거나 줄였다. 처음에 모든 Conv 층에 dropout을 주었는데 이것을 제거하고 성능이 올랐다
2)	batch normalization – 정규화 
 conv안에 dropout과 batch normalization을 적용시 conv – bn – relu – dropout – pool 이런 순서로 주는게 가장 효과적이라고 블로그에서 봐서 이렇게 적용을 했다. Layer를 통해 들어오는 입력을 Batchnorm을 통해 정규화를 하여 학습을 안정화시키는 효과가 있다. 처음에는 Conv층을 conv- relu-pool만 주어서 성능이 50프로였지만 batchnorm을 주고 성능이 50프로 정도 올라갔다.
3)	regularization – weight_decay
과적합을 막는데 가장 좋았던 방법은 regularization이였다. 손실함수에 규제를 적용하여 오버피팅을 막는 기법이다. Weight_decay 값이 커질수록 규제가 커지는 것이고 작을수록 작아지는 것이다, 그러나 너무 강하게 규제를 주면 모델이 과하게 규제되어 언더피팅이 나게 된다. 실제로 실험할 때 weight_decay의 적절한 값을 찾는데 어려움이 많았다.
4)	Ensemble
성능을 올리는데 가장 효과를 보았던 것은 앙상블이 였다. 앙상블은 여러 모델 결과를 모아서 최고의 성능을 내는 것이다. 앙상블 기법이 다양하지만 그 중에서 Soft voting을 사용하였다. Soft voting은 다수의 모델에서 예측한 확률 값을 한 곳으로 모아 더해 확률 기반으로 예측을 하는 것이다. 코드는 다음과 같다. Model.eval()을 통해서 모델의 각각의 확률 값을 도출해 내고 그 모든 것을 더하여 softmax에 넣어주었다. 이렇게 하면 모델 중 하나가 잘못 예측을 해도 나머지 학습이 잘 된 3개가 상쇄를 시켜준다.앙상블을 하여 5프로 정도 올랐다.  
가중치 앙상블도 해보았다. 각각 모델에 가중치를 주어 어떤 모델은 영향을 많이 미치게 하고 어떤 모델은 낮추는 것이다. 이것은 실제로 효과를 보지 못하였다.
5)	Label_smoothing

실험을 하면서 항상 confusion matrix를 뽑아서 보면 30대를 구분을 잘 하지 못하는 결과가 나왔었다.아래의 그림을 보면 유독 30대가 0.016으로 낮은 확률인 것을 볼 수 있다.
처음에는 label smoothing을 주어서 저 label의 정확도가 올라갔다고 생각을 했다. 하지만 label smoothing은 모델이 너무 정답을 확신하지 않고 다른 레이블도 보게 함으로써 일반화 성능을 올리게 하는 것이다. 즉 과적합이 방지 될 수 있다.아래 그림을 보면 실제로 레이블의 확률 값이 높아졌음을 확인이 가능하다. label smoothing시 적용시 다른 레이블도 고려를 하면서 자연스럽게 30~41대 label의 확률도 올라 같던 것 같다. 실제로 성능이 올랐던 이유는 과적합이 막아지면서 loss가 낮아져서 성능이 올랐다고 느꼈던 것 같다.

